{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Data Preprocessing is a technique that is used to convert the raw data into a clean data set. It is necessary because of the presence of unformatted real world data, which may not be supported by the desired ML/DL algorithm. \n",
    "\n",
    "In other words, whenever the data is gathered from different sources, it is collected in raw format which is not feasible for the analysis. Certain steps are executed to convert the data into a small clean data set, among which **Feature extraction**, **Data Cleaning**, **Feature selection and transformation**.\n",
    "\n",
    "### Definitions\n",
    "\n",
    "Before proceeding to data preprocessing, some definitions are in need.\n",
    "\n",
    "An **instance**, or **example** or **feature-vector**, is a transaction, an entry of a table. It is composed by different fields, called **features** or **dimensions** or **attributes**.\n",
    "\n",
    "An instance can be **dense**, if the number of 0s or NULL values is low, or **sparse**, if most of the features are non significant for that instance.\n",
    "\n",
    "![instance_types](../images/instance_types.png)\n",
    "\n",
    "A feature can also have different types:\n",
    "\n",
    "![feature_types](../images/feature_types.png)\n",
    "\n",
    "According to attribute/instance type, different kinds or relationships (**patterns**) can be extracted, with different algorithms. \n",
    "\n",
    "With feature analysis, **classification** algorithms can predict the value of a discrete attribute, while **regression** algorithm can predict the value of a numeric attribute.\n",
    "\n",
    "With instance analysis, **clustering** algorithms can determine a subset of rows with similar feature values, while **outlier detection** can determine **noise**, which means rows that are very different from the other rows.\n",
    "\n",
    "### Feature extraction\n",
    "Different sources generate different kinds of data, structured or not, from which features and therefore information have to be extracted:\n",
    "\n",
    "- sensor data - wavelets or Fourier transform\n",
    "- image data - histogram or visual words\n",
    "- web logs - multidimensional data\n",
    "- network traffic - procotol-specific, bytes transferred\n",
    "- text data - documents, tweets, multidimensional data\n",
    "\n",
    "Each of those has its own way to be converted to a more useful datatype, according to the type of feature to input into the classifier:\n",
    "\n",
    "- numeric to discrete {equi-width ranges, equi-log ranges, equi-depth ranges}\n",
    "- discrete to numeric (binarization)\n",
    "- text to numeric {stop word removal, **tf-idf**, multidimensional data}\n",
    "- time series to discrete sequence data (SAX: equi-depth discretization after window-based averaging)\n",
    "- time series to numeric data {discrete wavelet transform, discrete Fourier transform}\n",
    "\n",
    "![tfidf](../images/tdidf.png)\n",
    "\n",
    "### Data Cleaning \n",
    "\n",
    "Real world data is mostly composed of:\n",
    "\n",
    "- Inaccurate data (**missing data**) - There are many reasons for missing data such as data is not continuously collected, a mistake in data entry, technical problems with biometrics and much more.\n",
    "- The presence of **noisy data** (erroneous data and outliers) - The reasons for the existence of noisy data could be a technological problem of gadget that gathers data, a human mistake during data entry and much more.\n",
    "- **Inconsistent data** - The presence of inconsistencies are due to the reasons such that existence of duplication within data, human data entry, containing mistakes in codes or names, i.e., violation of data constraints and much more.\n",
    "\n",
    "These inconsistencies are to be taken care of before moving on to the analysis phase.\n",
    "\n",
    "**Missing data** implies incomplete data: the option are to ignore those entries, to estimate the missing values, or use algorithms that can handle missing values.\n",
    "\n",
    "**Noise** can be easily dealt with thanks to **clustering** algorithms or *binning*.\n",
    "\n",
    "**Incorrect entries** are more tricky to handle, due to their very different nature: there can be duplicates, inconsistencies, mistakes, etc. Generally, domain knowledge can give a better understanding of these errors and how to deal with them. Alternatively, data-centric methods can be used, which deal with inconsistencies.\n",
    "\n",
    "Data belonging to the same domani but with different unit of measure (such as currency for example) should be **standardized** or **normalized**: \n",
    "\n",
    "![stand_norm.png](../images/stand_norm.png)\n",
    "\n",
    "### Feature Selection and Transformation\n",
    "\n",
    "Once the data is clean and ready for analysis, it may include many features that do not concern our analysis. The process of **feature selection** selects a subset of relevant features for use in the model construction. The goals are four:\n",
    "\n",
    "- simplification of models\n",
    "- shorter training time\n",
    "- avoid the *curse of dimensionality*\n",
    "- reduce *overfitting* (reduction of *variance*)\n",
    "\n",
    "**Sampling** the data is one of the possible approaches to reduce the dataset. It consists of taking just a portion of the original dataset, but with all of its feature. It can be compared to SQL *where* clause, filtering a set of instances. \n",
    "\n",
    "Static data have many ways to be sampled, such as *sampling with/without replacement*, *biased sampling*, *stratified sampling*. Data streams instead are to be handled with the *Reservoir Sampling* method: \n",
    "\n",
    "> Given a data stream, choose *k* items with the same probability, storing only *k* elements in memory.\n",
    "\n",
    "![res_sampl_alg.png](../images/res_sampl_alg.png)\n",
    "\n",
    "An alternative approach is the **feature subset selection**. It consists of selecting some of the subsets of an instance, but for every instance of the starting dataset. It can be compared to the SQL *select* operator. \n",
    "\n",
    "There are different types of feature subset selection: *supervised f.s.*, *unsupervised f.s.*, *biased sampling*, *stratified sampling*.\n",
    "\n",
    "A last approach could be looking at data from a different point of view, in order to **reduce the dimensionality**: it's the main concept behind some algorithms, such as *PCA (Principal Component Analysis)*, *SVD (Single Value Decomposition)*, *LSA (Latent Semantic Analysis)*.\n",
    "\n",
    "Among those, **PCA** is probably the most known. It computes the most meaningful basis to re-express a noisy, confused dataset. By computing a new basis, it could be possible to filter out the noise and reveal hidden dynamics. Just as if data were to be viewed from another perspective. A PCA algorithm follows these steps:\n",
    "\n",
    "![pca_alg](../images/pca_alg.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
