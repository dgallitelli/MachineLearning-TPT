{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Classification is defined as such:\n",
    "\n",
    "> Given *n<sub>c</sub>* different classes, a classifier algorithm builds a model that **predicts** for every unlabelled instance *i* the class *C* to which it belongs, with a certain degree of accuracy.\n",
    "\n",
    "In other words, we build a classification model in order to predict a *label* given an *instance*, where *label* is how we classify a certain record (yes/no are an example of binomial labels, and a record can be classified according to whether it belongs or not to a certain category) and *instance* is a table record, containing information regarding the element we are analyzing. \n",
    "\n",
    "An instance is defined by **features**, which are what the classifier uses to decide how to classify the instance.\n",
    "\n",
    "![class_example](../images/classification_example.png)\n",
    "\n",
    "Many different kinds of classifier exist. Most of them can be found in the python library **scikit-learn**, which can be imported by simply running\n",
    "\n",
    "> from sklearn import <what-you-need>\n",
    "\n",
    "Most scikit-learn classifiers implement two methods:\n",
    "\n",
    "- **fit(train_data, train_label)** - method for the training phase, it accepts as argument a training dataset on which it applies an algorithm defined in the *fit* method itself.\n",
    "- **predict(test_data)** - method for the prediction phase, applies the previous algorithm to the test_data in order to predict its labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Class Classifier\n",
    "The **majority class classifier** is the most basic classifier:\n",
    "\n",
    "- in the **training phase**, it computes the majority class of the dataset - the label that appears the most in the training dataset\n",
    "- in the **prediction phase**, it ouputs the majority class of the dataset - outputs an array containing as many values as the test datasets, and all of them have value equal to the label previously found in the training phase.\n",
    "\n",
    "This is implemented in the *sickit-learn* library via the **DummyClassifier**, with **strategy='most_frequent'** option.\n",
    "\n",
    "```python\n",
    "from sklearn.dummy import DummyClassifier\n",
    "mjclass = DummyClassifier(strategy=\"most_frequent\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbours [k-NN Classifier]\n",
    "\n",
    "In k-NN classifier, the *k* elements closest to the instance analysed are used to predict, by majority class, the label of the instance.\n",
    "\n",
    "- in the **training phase**, all instances are stored in memory\n",
    "    - it is not appropriate for big data, due to huge requirements in memory space\n",
    "- in the **prediction phase**, a majority classifier is applied on the k-nearest instances\n",
    "    - the choice of *k* is very important, and is best done by first inspecting the data\n",
    "        - large K value reduces overall *noise*\n",
    "        - generally K is between 3-10\n",
    "    - a definition of k-nearest is in need, as well as an algorithm to compute said distance (examples are *Euclidean d.*, *Manhattan d.*, *Minkowski d.*, *Hamming d.* (only for categorical variables))\n",
    "    - distance computation can be computationally intensive\n",
    "    \n",
    "#### Implementation\n",
    "    \n",
    "The k-NN classifier is implemented in the *scikit-learn* library in **sklearn.neighbors**, which contain **KNeighborsClassifier**:\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnClass = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes\n",
    "\n",
    "Naïve Bayes classifiers are a family of simple probabilistic classifiers, based on applying Bayes' theorem, but with strong independence assumptions between the features - the features should not influence each other in any way (low correlation among features).\n",
    "\n",
    "![nbtheorem](https://www.analyticsvidhya.com/wp-content/uploads/2015/09/Bayes_rule-300x172-300x172.png)\n",
    "\n",
    "- P(c|x) is the **posterior probability of class** (c, target) given predictor (x, attributes) - probability of the instance x to belong to class c given its *x<sub>n</sub>* attributes.\n",
    "- P(c) is the **prior probability of class**.\n",
    "- P(x|c) is the **likelihood** which is the **probability of predictor given class** - probability of an instance of class x to have the attributes *x<sub>n</sub>* like in instance x.\n",
    "- P(x) is the **prior probability of predictor**.\n",
    "\n",
    "What this classifier does is estimating the probability of observing attribute *x<sub>n</sub>* and the prior probability *P(c)*.\n",
    "\n",
    "#### Naïve Bayes algorithm: an example\n",
    "![nb_example](../images/nb_example.png)\n",
    "With the training dataset in the first image, we need to classify (AKA predict) whether players will play or not based on weather condition. A few easy steps can be followed to do that:\n",
    "\n",
    "1. convert the data from the default dataset (image1) to a frequency table (image2)\n",
    "2. build the Likelihood table (image3) by finding the probabilities of each weather (sum rows/columns and divide by total cases) - these are the P(x|c) of the Naive Bayes equation\n",
    "3. Calculate the posterior probability for each class with the Naive Bayes equation. The class with the highest posterior probability is the outcome of prediction.\n",
    "\n",
    "So, if we want to predict whether players will paly if the weather is sunny, our equation would become:\n",
    "\n",
    "> P(Yes|Sunny) = P(Sunny|Yes) \\* P(Yes)/P(Sunny) = 3/9 \\* 9/14 \\* 5/14 = 0.6\n",
    "\n",
    "#### Multinomial Naïve Bayes\n",
    "\n",
    "A particular case of Naïve Bayes classifier is the Multinomial Nave Bayes: it is used for document classification, because it computes the probability of a document *d* of being in class *c*. The document is considered as a bag-of-words: the estimation then becomes the probability of observing word *w* and the prior probability *P(c)*:\n",
    "\n",
    "![mnb_eq.png](../images/mnb_eq.png)\n",
    "\n",
    "where *n<sub>w<sub>d</sub></sub>* is the number of times the word *w* appears in the document *d*.\n",
    "\n",
    "#### Side Note: Laplacian Smoothing\n",
    "\n",
    "Sometimes, in queries there could be some words which are not in the vocabulary. This would generate a probability of 0 for the whole query, but that's clearly not true.\n",
    "\n",
    "In order to solve this problem, we introduce a correction in the probability computation, which is the **Laplacian correction**. Given a *k* factor, the correction is applied by:\n",
    "\n",
    "- summing *k* to the numerator and **k x #classes** to the denominator for the prior probability\n",
    "- summing *k* to the numerator and **#words in vocabulary** to the denominator for the likelihood.\n",
    "\n",
    "#### Pros/cons and applications\n",
    "\n",
    "Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods, if the assumption of independence holds. It perform well in case of categorical input variables compared to numerical variable(s).\n",
    "\n",
    "Naive Bayes algorithms are very useful for:\n",
    "\n",
    "- Real time prediction - it's very fast\n",
    "- Multi class prediction\n",
    "- Text classification/Spam filtering/Sentiment Analysis\n",
    "- Recommendation System\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "The **sklearn.naive_bayes** module implements Naïve Bayes algorithms in the *scikit-learn* library. There are a few different classifiers based on the Naïve Bayes algorithm, among which:\n",
    "\n",
    "- GaussianNB() - usually use this\n",
    "- BernoulliNB() - for multivariate Bernoulli models\n",
    "- MultinomialNB() - for multinomial models\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nbClass = GaussianNB()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron and Neural Networks\n",
    "\n",
    "### Perceptron / Neuron\n",
    "\n",
    "![neuron_schema.png](../images/neuron_schema.png)\n",
    "\n",
    "The perceptron is the very basic component of any artificial neural network. It is modeled after the behavior of the neurons in the brain, having a number of input channels, a processing core and an output channel. Three phases can be identified for how the processing work in a neuron:\n",
    "\n",
    "1. **weights** - a certain value in input, namely an attribute, is multiplied by a *weight* value that is assigned to the particular input; during the **learning phase**, the perceptron can adjust the weights based on the error of the last test result.\n",
    "2. **sum** - the weighted signals are summed up to a single value, applying an offset called *bias* which is also updated during the learning phase.\n",
    "3. **activation** - the result of the neuron's calculation is turned into an output signal by feeding it to a *activation function* (also called *transfer function*). Different functions can be used here, such as the *sign* function, the *step* function, or the *sigmoid* function.\n",
    "\n",
    "Given an input *[x<sub>i</sub>, y<sub>i</sub>]*, where *x<sub>i</sub>* data vector and *y<sub>i</sub>* the label of the data, our goal is to minimize the error between output of the neuron (prediction) and the training label set. Generally, we want to minimize the **mean-square error** in order to increase our accuracy. This can be done by updating the set of weights, using **backward propagation of the error** from which we can compute the **weight update rule**:\n",
    "\n",
    "![perc_update.png](../images/perc_update.png)\n",
    "\n",
    "The stopping condition for the weight update can be either a threshold of iterations specified at the beginning, or just a non-improving criteria.\n",
    "\n",
    "### Artificial Neural Network\n",
    "\n",
    "![Artificial_Neural_Network](../images/ann.png)\n",
    "\n",
    "An artificial neural network is basically a network of perceptron, where a layer of neurons is fed by the previous layer, starting from the input data until the final result is obtained. Three main layers can be identified:\n",
    "\n",
    "- **input layer**, the first layer of neurons, which are fed the input features vector and propagate their result to the next layer\n",
    "- **hidden layer(s)**, one or more level of neurons, which keep propagating their results from layer to layer, until the final layer\n",
    "- **output layer**, which is the last layer of neurons, which must generate the prediction.\n",
    "\n",
    "### Recurrent Neural Network\n",
    "\n",
    "Traditional perceptron, and therefore neural networks, do not have no state persistence. The goal of RNN is to address this issue, by inserting loops in perceptron to allow persistence of information.\n",
    "\n",
    "A RNN can be thought of as multiple copies of the same network, each passing a message to a successor. The unrolling of a recurrent neural network could look as follows:\n",
    "\n",
    "![Recurrent_neural_network](../images/rnn.png)\n",
    "\n",
    "### Pros/cons and Applications\n",
    "\n",
    "Neural networks are really powerful. They are easy to use and don't require any parameters definition. But, it is a *black-box* learning model, meaning that no easy interpretation between input and output can be understood from the model itself. Moreover, reaching convergence to a good result, especially with feature-rich data and many layers, can be quite both time and resource intensive. \n",
    "\n",
    "Neural networks have been used on a variety of tasks, including **computer vision**, **speech recognition**, **machine translation**, social network filtering, playing board and video games, **medical diagnosis** and in many other domains.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Artificial neural network are included in the *scikit* library under the name of **Multi-layer Perceptron**. It can be used for classification purposes by importing **MLPClassifier** from the **sklearn.neural_network**.\n",
    "\n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlpClass = MLPClassifier(hidden_layer_sizes=(5, 2) \n",
    "#5 neurons on first hidden layer, 2 neurons on the second.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "Decision tree learning is based on building a structure (the **decision tree**) to classify an instance by splitting features according to their value, according to an **induction strategy**. Feature value allow to decide the label for the test instance, but in order to do so a splitting criteria has to be decided, such as *Gini impurity*, *information gain*, *variance*.\n",
    "\n",
    "An example is the following, about the survival of passengers on the Titanic:\n",
    "\n",
    "![titanic_tree](https://upload.wikimedia.org/wikipedia/commons/6/6b/Titanic_Survival_Decison_Tree_SVG.png)\n",
    "\n",
    "### Ensemble techniques\n",
    "Often, a decision tree is not enough to have a good accuracy. Ensemble methods are used to build more than one tree and join the results.\n",
    "\n",
    "The **bagging/bootstrap** technique build a set of *M* base model, by picking random samples with replacement from a dataset of instances. In a nutshell, it generates *M* datasets in order to obtain *M* different classifier models.\n",
    "\n",
    "![dt_bagging.png](../images/dt_bagging.png)\n",
    "\n",
    "A specific type of bagging is **random forest**, one of the most popular classification algorithms. After a phase of bagging, it builds random trees which only use a random subset of the attributes. This combination of methods allows to generate a very accurate model, which generally beats any other model.\n",
    "\n",
    "![random_forest.png](../images/random_forest.png)\n",
    "\n",
    "Finally, **boosting** is based on the concept of transorming a weak learner into a strong one: it takes the previously built model and tries to improve it incrementally. An example of this method is *AdaBoost*.\n",
    "\n",
    "### Pros/cons and Applications\n",
    "\n",
    "A decision tree model is really easy to understand and interpret. It handles pretty well both numerical and categorical data, unlike other models (e.g. neural networks). Decision trees also requires very little data preparation: trees can handle qualitative predictors, therefore not needing data normalization. It performs extremely well with large datasets.\n",
    "\n",
    "Trees are generally very non-robust, meaning that a slight change in data can bring a big change in the tree and consequently in the final prediction. Decision tree learners can incur into **overfitting**: the model created may be too specific for the problem at hand and not generalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Stacking (also called meta ensembling) is a model ensembling technique used to combine information from multiple predictive models to generate a new model. Often times the stacked model will outperform each of the individual models. For this reason, stacking is most effective when the base models are significantly different. \n",
    "\n",
    "Generally, random forest is preferred over boosting, which is preferred over stacking.\n",
    "\n",
    "Let's consider for example the [spam dataset](../images/classification_example.png), posted above in the [introduction](#classification). Let's suppose that three classifier have produced, for each instance, the following prediction.\n",
    "\n",
    "| Instance | True Label | Pred1 | Pred2 | Pred3 |\n",
    "|:--------:|:----------:|:-----:|:-----:|:-----:|\n",
    "|I1|Y|Y|N|Y|\n",
    "|I2|Y|N|N|Y|\n",
    "|I3|Y|Y|Y|Y|\n",
    "|I4|N|N|N|Y|\n",
    "|I5|N|N|Y|N|\n",
    "\n",
    "This is what is called **meta-dataset**. Finally, a **meta-classifier** has to be used to obtain a better prediction on incoming new instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Evaluation\n",
    "\n",
    "Building accurate classification models is necessary for accurate predictions. However, it is not clear how to evaluate the accuracy of a classification model.\n",
    "\n",
    "There are many different methods for classification evaluation:\n",
    "\n",
    "## Error estimation\n",
    "\n",
    "**Hold-out** is based on taking out a random and independent set of the available dataset and use it as a **test set** for the just built classification model.\n",
    "\n",
    "**k-Fold cross-validation** is based on the concept that, in machine learning, knowing data label is expensive and therefore all lebeled data should be used to build the model. In cross-validation, the training dataset is split in *k* parts called *folds*, and the model is tested on the *i*-th fold, with *i* from 0 to *k-1*, testing the model *k* times. Usually, a good value is *k=10*.\n",
    "\n",
    "![cross-validation.jpg](../images/cross-validation.jpg)\n",
    "\n",
    "## Performance Measures\n",
    "\n",
    "#### Confusion Matrix \n",
    "A **confusion matrix**, or **error matrix**, is a table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). It makes it easy to see if the model is milabelling instances, and the percentage of it.\n",
    "\n",
    "| | Prediction + | Prediction - | Total |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|Correct +|tp|fn|tp+fn|\n",
    "|Correct -|fp|tn|fp+tn|\n",
    "|Total|tp+fp|fn+tn|N|\n",
    "\n",
    "Some metrics can be derived from this table:\n",
    "\n",
    "- *Precision* = tp/(tp+fp)\n",
    "- *Recall* = tp/(tp+fn) [AKA **True Prediction Rate**]\n",
    "- *F1-Measure* = 2 \\* (precision \\* recall)/(precision+recall)\n",
    "- *Accuracy* = (tp+tn)/N\n",
    "- *Arithmetic mean* = (tp/(tp+fn) + tn/(fp+tn))/2\n",
    "- *Geometric mean* = sqrt(tp/(tp+fn) \\* tn/(fp+tn))\n",
    "\n",
    "#### Kappa Statistic\n",
    "\n",
    "![kappa_stat.png](../images/kappa_stat.png)\n",
    "\n",
    "#### Matthews correlation coefficient (MCC)\n",
    "\n",
    "![MCC.png](../images/mcc.png)\n",
    "\n",
    "#### Area Under the Curve (AUC)\n",
    "\n",
    "A ROC space is defined by FPR = fp/(fp+tp) and TPR = tp/(tp+fn)\n",
    "\n",
    "![ROC_curve.png](../images/ROC_curve.png)\n",
    "\n",
    "## Statistical significance validation\n",
    "\n",
    "In case 2 classifiers are built, their performance should be evaluated with respect to one another.\n",
    "\n",
    "#### McNemar Test (2 classifiers)\n",
    "\n",
    "![mcnemar.png](../images/mcnemar.png)\n",
    "\n",
    "#### Nemenyi Test (>2 classifiers)\n",
    "\n",
    "![nemenyi.png](../images/nemenyi.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
