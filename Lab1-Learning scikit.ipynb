{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1 - Getting started with scikit-learn for Machine Learning\n",
    "\n",
    "Scikit-learn is the leading machine learning software in Python. It is a project started in Paris, Inria and Telecom ParisTech. It is easy to use and extend.\n",
    "\n",
    "A basic scikit-learn Start Tutorial is available [here](http://scikit-learn.org/stable/tutorial/basic/tutorial.html).\n",
    "\n",
    "The goal of this first notebook is to get used with the scikit-learn library. We will load the iris dataset, a well-known dataset for machine learning, and split it into a training dataset and a test dataset. The first one will be used to fit a **k-nearest-neighbour classifier**, while the second one is used to test the just built model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to **import** the libraries needed. The *sklearn* library provides the iris dataset, which is loaded by means of the *load_iris()* method. The *numpy* library provides the methods *unique*, which returns the sorted unique elements of an array - in our case, it shows that there are only three classes as possible outputs [0, 1, 2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary classes\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load and parse the data file\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "iris_Y = iris.target\n",
    "np.unique(iris_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the initial data are ready, we can start defining the **training** and **testing** datasets, starting from the original iris dataset. First a random seed is initialized, then a random permutation is applied on the elements of the iris_X instance of the iris dataset - obtaining a shuffled version of the iris dataset.\n",
    "Then, we generate 4 sub-datasets:\n",
    "\n",
    "- **iris_X_train** and **iris_Y_train**: taking all elements from the beginning, minus the last 10\n",
    "- **iris_X_test** and **iris_Y_test**: taking the last 10 elements of the array\n",
    "\n",
    "**NB:** Python arrays support the following syntax:\n",
    "    \n",
    "    array2 = array[start_point:end_point]\n",
    "    \n",
    "This code assigns to *array2* all elements from *start_point* to *end_point* from the source *array*. This syntax also implicitly defines 0 as *start_point* and *len(array)* if nothing is specified respectively before and after the *:* . Negative start/end points (i.e.: -X) stand for X elements before the end of the array: **[-10:]** means \"*the last 10 elements of the array*\", while **[:-10]** means \"*all elements from the beginning excluding the last 10*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split iris data in train and test data\n",
    "#Â A random permutation, to split the data randomly\n",
    "np.random.seed ( 0 )\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "# Take some elements from the shuffled array\n",
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_Y_train = iris_Y[indices[:-10]]\n",
    "iris_X_test = iris_X[indices[-10:]]\n",
    "iris_Y_test = iris_Y[indices[-10:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create and fit a **k-nearest-neighbour classifier**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(iris_X_train, iris_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model on test instances and compute test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "iris_prediction = knn.predict(iris_X_train)\n",
    "iris_prediction\n",
    "iris_Y_test\n",
    "accuracy_score(iris_Y_test, knn.predict(iris_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
